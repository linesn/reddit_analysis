{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Copy of Reddit_Search-1.0.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linesn/reddit_analysis/blob/main/Reddit_Search_NL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "ZM-O8R0_xQwQ"
      },
      "source": [
        "# Gathering Reddit political posts on Biden\n",
        "*Nick Lines*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9E5rJ4HxQwX"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is an adaptation of a notebook provided by the Discovery Lab. License details for my use follow.\n",
        "\n",
        "The intention of this script is to allow the user to download a corpus of recent Reddit comments from a set of reference and political subredits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv_BZulRxQwY"
      },
      "source": [
        "Licence\n",
        "-------------\n",
        "Developed by the Discovery Lab, Applied Intelligence Group, Accenture Federal Systems.\n",
        "\n",
        "```\n",
        "Copyright (c) 2020 Accenture Federal Systems.\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```\n",
        "\n",
        "\n",
        "Purpose\n",
        "-------------\n",
        "This notebook collects posts and comments, and associated metadata from the [Reddit](https://www.reddit.com/) social media platform. It uses Reddit API through [PRAW](https://praw.readthedocs.io/en/latest/) and requires credentials.\n",
        "\n",
        "\n",
        "Input\n",
        "-------------\n",
        "**Required Parameters**\n",
        "\n",
        "- _**client\\_id**_ (string) - A unique client id provided by Reddit.\n",
        "\n",
        "- _**client\\_secret**_ (string) - Secret associated with client id provided by Reddit.\n",
        "\n",
        "- _**user\\_agent**_ (string) - A unique user agent provided by Reddit.\n",
        "\n",
        "- _**search\\_terms**_ (array of strings) - Search terms.\n",
        "\n",
        "- _**subreddits**_ (array of strings) - Names of subreddits to search from.\n",
        "\n",
        "\n",
        "**Optional Parameters**\n",
        "\n",
        "- _**post\\_limit**_ (integer, default: 100, maximum: 1000) - Maximum number of posts to download.\n",
        "\n",
        "\n",
        "Output\n",
        "-------------\n",
        "The outputs are two `CSV` files named `REDDIT_POSTS_{{DATETIME}}.csv` and `REDDIT_COMMENTS_{{DATETIME}}.csv`, where `{{DATETIME}}` is the approximate date/time of the data collection. These `CSV` files are saved in the `{{HOME}}/data/raw/Reddit` folder, where `{{HOME}}` is the project installation folder.\n",
        "\n",
        "The columns of the output file `REDDIT_POSTS_{{DATETIME}}.csv` is as follows.\n",
        "\n",
        "- _**author**_ (string) -  Provides an instance of `Redditor`.\n",
        "\n",
        "- _**clicked**_ (binary) -  Whether or not the submission has been clicked by the client. \n",
        "\n",
        "- _**comments**_ (array of strings) -  Provides an instance of `CommentForest`. \n",
        "\n",
        "- _**created_utc**_ (datetime) - Time the submission was created, represented in Unix Time. \n",
        "\n",
        "- _**distinguished**_ (binary) - Whether or not the submission is distinguished. \n",
        "\n",
        "- _**edited**_ (binary) - Whether or not the submission has been edited. \n",
        "\n",
        "- _**id**_ (string) - ID of the submission. \n",
        "\n",
        "- _**is\\_original_content**_ (binary) - Whether or not the submission has been set as original content. \n",
        "\n",
        "- _**is\\_self**_ (binary) - Whether or not the submission is a selfpost (text-only). \n",
        "\n",
        "- _**link\\_flair_template\\_id**_ (string) - The link flair’s ID, or None if not flaired. \n",
        "\n",
        "- _**link\\_flair\\_text**_ (text) - The link flair’s text content, or None if not flaired. \n",
        "\n",
        "- _**locked**_ (binary) - Whether or not the submission has been locked. \n",
        "\n",
        "- _**name**_ (string) - Fullname of the submission. \n",
        "\n",
        "- _**num\\_comments**_ (integer) - The number of comments on the submission. \n",
        "\n",
        "- _**over\\_18**_ (binary) - Whether or not the submission has been marked as NSFW. \n",
        "\n",
        "- _**permalink**_ (string) - A permalink for the submission. \n",
        "\n",
        "- _**poll\\_data**_ (object) - A PollData object representing the data of this submission, if it is a poll submission. \n",
        "\n",
        "- _**score**_ (integer) - The number of upvotes for the submission. \n",
        "\n",
        "- _**selftext**_ (text) - The submissions’ selftext - an empty string if a link post. \n",
        "\n",
        "- _**spoiler**_ (binary) - Whether or not the submission has been marked as a spoiler. \n",
        "\n",
        "- _**stickied**_ (binary) - Whether or not the submission is stickied. \n",
        "\n",
        "- _**subreddit**_ (string) - Provides an instance of Subreddit. \n",
        "\n",
        "- _**title**_ (text) - The title of the submission. \n",
        "\n",
        "- _**upvote\\_ratio**_ (double) - The percentage of upvotes from all votes on the submission. \n",
        "\n",
        "- _**url**_ (string) - The URL the submission links to, or the permalink if a selfpost. \n",
        "\n",
        "\n",
        "The columns of the output file `REDDIT_COMMENTS_{{DATETIME}}.csv` is as follows.\n",
        "\n",
        "- _**author**_ (string) - Provides an instance of Redditor. \n",
        "\n",
        "- _**body**_ (text) -  The body of the comment, as Markdown.\n",
        "\n",
        "- _**body\\_html**_ (text) - The body of the comment, as HTML.\n",
        "\n",
        "- _**created\\_utc**_ (datetime) - Time the comment was created, represented in Unix Time. \n",
        "\n",
        "- _**distinguished**_ (binary) - Whether or not the comment is distinguished. \n",
        "\n",
        "- _**edited**_ (binary) - Whether or not the comment has been edited. \n",
        "\n",
        "- _**id**_ (string) - ID of the comment. \n",
        "\n",
        "- _**is\\_submitter**_ (binary) - Whether or not the comment author is also the author of the submission. \n",
        "\n",
        "- _**link\\_id**_ (string) - The submission ID that the comment belongs to. \n",
        "\n",
        "- _**parent\\_id**_ (string) - The ID of the parent comment (prefixed with t1\\_). If it is a top-level comment, this returns the submission ID instead (prefixed with t3\\_). \n",
        "\n",
        "- _**permalink**_ (string) - A permalink for the comment. Comment objects from the inbox have a context attribute instead. \n",
        "\n",
        "- _**replies**_ (integer) - Provides an instance of CommentForest. \n",
        "\n",
        "- _**score**_ (integer) - The number of upvotes for the comment. \n",
        "\n",
        "- _**stickied**_ (binary) - Whether or not the comment is stickied. \n",
        " \n",
        "- _**submission**_ (string) - Provides an instance of the submission that the comment belongs to. \n",
        "\n",
        "- _**subreddit**_ (string) - Provides an instance of the subreddit that the comment belongs to. \n",
        "\n",
        "- _**subreddit\\_id**_ (string) - The subreddit ID that the comment belongs to. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2drwQ2JvxQwZ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RiVi6oRxQwa"
      },
      "source": [
        "<p> The imports, function and class defintions, global variables, and system-dependent configuration are in this section. </p>\n",
        "\n",
        "<p> The system dependent configuration should be carefully reviewed and configured for each system (e.g., Linux vs. Windows, or the path of an external program) since the playbook will most likely fail without proper configuration. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IHb29AWxQwa"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0A70xzExvT7"
      },
      "source": [
        "# for scraping\n",
        "try:\n",
        "  from selenium import webdriver\n",
        "  from selenium.common.exceptions import StaleElementReferenceException\n",
        "  from selenium.webdriver.common.keys import Keys\n",
        "  from selenium.webdriver.chrome.options import Options\n",
        "  from selenium.webdriver.support.ui import WebDriverWait\n",
        "except:\n",
        "  !pip install selenium\n",
        "  from selenium import webdriver\n",
        "  from selenium.common.exceptions import StaleElementReferenceException\n",
        "  from selenium.webdriver.common.keys import Keys\n",
        "  from selenium.webdriver.chrome.options import Options\n",
        "  from selenium.webdriver.support.ui import WebDriverWait"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2vsTe9Bmjt2"
      },
      "source": [
        "If you use synchronous prawl, you may get overwhelmed by warnings to use asynchronous praw. Therefore, we may want to silence the warnings (not something you should usually do)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6AKkhDoyGOs"
      },
      "source": [
        "# For Reddit\n",
        "try:\n",
        "  import praw\n",
        "  from praw import Reddit\n",
        "  from praw.models import MoreComments\n",
        "except:\n",
        "  !pip install praw\n",
        "  import praw\n",
        "  from praw import Reddit\n",
        "  from praw.models import MoreComments  \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNxD0L_xxQwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4302ebed-dc34-4c1f-d8b8-0d34898c9070"
      },
      "source": [
        "\"\"\"This cell imports necessary Python modules and performs initial configuration\n",
        "\"\"\"\n",
        "\n",
        "# Data manipulation libraries\n",
        "import json\n",
        "import pandas as pd \n",
        "import csv\n",
        "\n",
        "\n",
        "# Visualization and Interaction\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.style.use('ggplot')\n",
        "from IPython.display import set_matplotlib_formats, display, clear_output, HTML\n",
        "set_matplotlib_formats('retina')\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
        "init_notebook_mode(connected=True)\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "from ipywidgets import VBox, HBox, Button, HTML, Label\n",
        "\n",
        "\n",
        "# Computation libraries \n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "\n",
        "# Graph analysis\n",
        "# import networkx as nx\n",
        "# import community\n",
        "\n",
        "\n",
        "# System related\n",
        "import io\n",
        "import platform\n",
        "from pathlib import Path\n",
        "import os\n",
        "from getpass import getpass\n",
        "# from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "# Datetime libraries\n",
        "from datetime import datetime\n",
        "import time\n",
        "from pytz import timezone\n",
        "\n",
        "# Scraping libraries\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Logging\n",
        "import logging \n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYroU-YexQwc"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e54Wcy8xQwd"
      },
      "source": [
        "\"\"\"This cell defines global variables and parameters used throughout the playbook\n",
        "\"\"\"\n",
        "\n",
        "# Set this to True if you want to watch Selenium scrape pages\n",
        "# WATCH_SCRAPING = True\n",
        "\n",
        "# Set this to True if you want to use incognito mode\n",
        "# USE_INCOGNITO = True\n",
        "\n",
        "# Number of posts \n",
        "post_limit = 1000\n",
        "\n",
        "# The data is written \n",
        "#RAW_DATA_DIRECTORY = Path(\"../data/raw/Reddit/\")\n",
        "\n",
        "# Setup logging level\n",
        "LOGGING_LEVEL = logging.INFO \n",
        "logging.basicConfig(level=LOGGING_LEVEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmrCdSIUxQwd"
      },
      "source": [
        "## Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f-NFbb6xQwd"
      },
      "source": [
        "\"\"\"This cell defines functions and classes used throughout the playbook\n",
        "\"\"\"\n",
        "\n",
        "def __init__(self, client_id, client_secret, user_agent, password):\n",
        "    self.client_id = client_id\n",
        "    self.client_secret = client_secret\n",
        "    self.user_agent = user_agent\n",
        "    self.password = password\n",
        "\n",
        "\n",
        "def token(client_id, client_secret, user_agent):\n",
        "    reddit = Reddit(client_id=client_id,\n",
        "                         client_secret=client_secret,\n",
        "                         user_agent=user_agent,\n",
        "                         check_for_async=False,\n",
        "                    )\n",
        "    if (reddit != False):\n",
        "        print(\"Successful token\")\n",
        "    else:\n",
        "        print(\"Failed token\")\n",
        "    return reddit\n",
        "\n",
        "\n",
        "def search_reddit(reddit, search_term, sort_type, time_limit, post_limit):\n",
        "    \"\"\"\n",
        "    GRAB REDDIT POSTS BY SEARCH TERM\n",
        "    search_term = any boolean search #https://www.reddit.com/dev/api/\n",
        "    sort_type = 'relevance', 'hot', 'top', 'new', 'comments'\n",
        "    time_limit = 'all', 'hour', 'day', 'week', 'month', 'year'\n",
        "    post_limit = 1000 maximum\n",
        "    \"\"\"\n",
        "\n",
        "    posts = []\n",
        "    subreddit = reddit.subreddit(\"all\")\n",
        "    for post in subreddit.search(search_term, sort=sort_type, time_filter=time_limit, limit=post_limit):\n",
        "        posts.append([post.subreddit, post.id, post.title, post.selftext, post.author, post.url, post.permalink,\n",
        "                      post.num_comments, post.created, post.score, post.distinguished, post.is_original_content,\n",
        "                      post.upvote_ratio, post.link_flair_text])\n",
        "    posts = pd.DataFrame(posts,\n",
        "                         columns=['subreddit', 'post_id', 'title', 'post_body', 'post_author', 'url', 'post_permalink',\n",
        "                                  'num_comments', 'post_created', 'post_score', 'post_distinguished',\n",
        "                                  'original_content', 'upvote_ratio', 'flair_text'])\n",
        "    posts['post_created'] = pd.to_datetime(posts['post_created'], unit='s')\n",
        "    posts['scrape_time'] = datetime.now()\n",
        "    posts[['subreddit', 'post_id', 'title', 'post_author',\n",
        "           'post_body', 'url', 'post_permalink', 'flair_text']] = posts[['subreddit', 'post_id', 'title', 'post_author',\n",
        "                                                                         'post_body', 'url', 'post_permalink',\n",
        "                                                                         'flair_text']].astype(str)\n",
        "    return posts\n",
        "\n",
        "\n",
        "def get_subreddit(reddit, sub, sort_type, time_limit, post_limit):\n",
        "    '''\n",
        "    GRAB REDDIT POSTS BY SUBREDDIT\n",
        "    sub = subreddits\n",
        "    sort_type = 'hot', 'top', 'new', 'gilded', 'rising', 'controversial'\n",
        "    time_limit = 'all', 'hour', 'day', 'week', 'month', 'year'\n",
        "    post_limit = 1000 maximum\n",
        "    '''\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    posts = []\n",
        "    if (sort_type == \"top\") or (sort_type == \"hot\") or (sort_type == \"controversial\"):\n",
        "        for post in subreddit.top(time_filter=time_limit, limit=post_limit):\n",
        "            posts.append([post.subreddit, post.id, post.title, post.selftext, post.author, post.url, post.permalink,\n",
        "                          post.num_comments, post.created, post.score, post.distinguished, post.is_original_content,\n",
        "                          post.upvote_ratio, post.link_flair_text])\n",
        "\n",
        "    if (sort_type == \"new\") or (sort_type == \"rising\"):\n",
        "        for post in subreddit.new(limit=post_limit):\n",
        "            posts.append([post.subreddit, post.id, post.title, post.selftext, post.author, post.url, post.permalink,\n",
        "                          post.num_comments, post.created, post.score, post.distinguished, post.is_original_content,\n",
        "                          post.upvote_ratio, post.link_flair_text])\n",
        "\n",
        "    posts = pd.DataFrame(posts,\n",
        "                         columns=['subreddit', 'post_id', 'title', 'post_body', 'post_author', 'url', 'post_permalink',\n",
        "                                  'num_comments', 'post_created', 'post_score', 'post_distinguished',\n",
        "                                  'original_content', 'upvote_ratio', 'flair_text'])\n",
        "    posts['post_created'] = pd.to_datetime(posts['post_created'], unit='s')\n",
        "    posts['scrape_time'] = datetime.now()\n",
        "    posts[['subreddit', 'post_id', 'title', 'post_author',\n",
        "           'post_body', 'url', 'post_permalink', 'flair_text', ]] = posts[\n",
        "        ['subreddit', 'post_id', 'title', 'post_author',\n",
        "         'post_body', 'url', 'post_permalink', 'flair_text']].astype(str)\n",
        "    return posts\n",
        "\n",
        "\n",
        "def get_reddit_comments(reddit, post_id):\n",
        "    submission = reddit.submission(id=post_id)\n",
        "    comment = []\n",
        "    for top_level in submission.comments:\n",
        "        if isinstance(top_level, MoreComments):\n",
        "            continue\n",
        "        comment.append([top_level.subreddit, top_level.submission, top_level.id, top_level.parent_id, top_level.author,\n",
        "                        top_level.permalink, top_level.body, top_level.created, top_level.score,\n",
        "                        top_level.distinguished])\n",
        "    comments = pd.DataFrame(comment, columns=['subreddit', 'post_id', 'comment_id', 'parent_id', 'comment_author',\n",
        "                                              'comment_permalink', 'comment_body', 'comment_created', 'comment_score',\n",
        "                                              'comment_distinguished'])\n",
        "    comments['comment_created'] = pd.to_datetime(comments['comment_created'], unit='s')\n",
        "    comments['scrape_time'] = datetime.now()\n",
        "    comments[['subreddit', 'post_id', 'comment_id', 'parent_id',\n",
        "              'comment_author', 'comment_permalink', 'comment_body']] = comments[\n",
        "        ['subreddit', 'post_id', 'comment_id', 'parent_id',\n",
        "         'comment_author', 'comment_permalink', 'comment_body']].astype(str)\n",
        "    return comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkuf1i6KxQwh"
      },
      "source": [
        "## System-dependent Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpln7s3jxQwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a96b4f-fefa-477a-c4be-213fe5b2ee66"
      },
      "source": [
        "\"\"\"This cell defines system-dependent configuration such as those different in Linux vs. Windows\n",
        "\"\"\"\n",
        "if 'COLAB_GPU' in os.environ: # a hacky way of determining if you are in colab.\n",
        "  print(\"Notebook is running in colab\")\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "  OUTPUT_DIR = \"./drive/My Drive/Data/raw/\"\n",
        "  RAW_DATA_DIRECTORY = Path(\"./drive/My Drive/Data/raw/Reddit/\")\n",
        "  \n",
        "else:\n",
        "  # Get the system information from the OS\n",
        "  PLATFORM_SYSTEM = platform.system()\n",
        "\n",
        "  # Darwin is macOS\n",
        "  if PLATFORM_SYSTEM == \"Darwin\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver\")\n",
        "  elif PLATFORM_SYSTEM == \"Windows\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver.exe\")\n",
        "  else:\n",
        "      logging.critical(\"Chromedriver not found or Chromedriver is outdated...\")\n",
        "      exit()\n",
        "  RAW_DATA_DIRECTORY = Path(\"../Data/raw/Reddit/\")\n",
        "\n",
        "os.makedirs(RAW_DATA_DIRECTORY, exist_ok=True)\n",
        "tz = timezone('US/Eastern')    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Notebook is running in colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6od59hoxQwk"
      },
      "source": [
        "# Collect Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxvYdRAFxQwm"
      },
      "source": [
        "## Collect Reddit Submissions and Comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfSgEtoCxQwm"
      },
      "source": [
        "\"\"\"This cell retrieves page posts and comments, for a given page.\n",
        "\"\"\"\n",
        "\n",
        "def collect(input_subreddits=None, input_search_terms=None, filename=\"\"):\n",
        "    # Credentials (create client_id, client_secret, user_agent by following \n",
        "    # https://praw.readthedocs.io/en/latest/getting_started/quick_start.html)\n",
        "    #client_id = getpass(\"Enter client_id: \") \n",
        "    client_id = \"zZ50cezaVccRCA\"\n",
        "    client_secret = getpass(\"Enter client secret: \")\n",
        "    #client_secret = \"\" # if you feel comfortable including it\n",
        "    #user_agent = input(\"Enter user agent: \")\n",
        "    user_agent = \"PostMasterGeneral\"\n",
        "\n",
        "    ''' Designate input parameters for functions\n",
        "    search terms = key terms to search ALL of reddit\n",
        "    subreddits = subreddits to collect\n",
        "    search_sort_type = 'relevance', 'hot', 'top', 'new', 'comments'\n",
        "    sub_sort_type = 'hot', 'top', 'new', 'gilded', 'rising', 'controversial'\n",
        "    time_limit = 'all', 'hour', 'day', 'week', 'month', 'year'\n",
        "    post_limit = 10\n",
        "    '''\n",
        "    post_limit = 1000 # maximum of 1000\n",
        "    sub_sort_type = 'new'  # , 'top', 'new', 'gilded', 'rising', 'controversial'\n",
        "    search_sort_type = 'new'  # , 'hot', 'top', 'new', 'comments', 'relevance'\n",
        "    time_limit = 'year'  # , 'hour', 'day', 'week', 'month', 'year'\n",
        "    \n",
        "    # search_terms = [\"covid19\", \"coronavirus\"]\n",
        "    # subreddits = [\"CoronavirusMemes\", \"Coronavirus\", \"CoronavirusUS\"]\n",
        "    if input_search_terms is None:\n",
        "      input_search_terms = input(\"Enter search terms (seperated by spaces): \")\n",
        "    search_terms = input_search_terms.split()\n",
        "    if input_subreddits is None:\n",
        "      input_subreddits = input(\"Enter subreddits (separated by spaces): \")\n",
        "    subreddits = input_subreddits.split()\n",
        "    \n",
        "    ''' Collect posts & corresponding comments\n",
        "    '''\n",
        "    # Create client\n",
        "    r = token(client_id, client_secret, user_agent)\n",
        "    list_posts_df = []\n",
        "    try:\n",
        "        for query in search_terms:\n",
        "            post_df = search_reddit(r, query, search_sort_type, time_limit, post_limit)\n",
        "            print(\"Grabbed\", len(post_df), \"posts with search term:\", query)\n",
        "            list_posts_df.append(post_df)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        for sub in subreddits:\n",
        "            post_df = get_subreddit(r, sub, sub_sort_type, time_limit, post_limit)\n",
        "            print(\"Grabbed\", len(post_df), \"posts from subreddit:\", sub)\n",
        "            list_posts_df.append(post_df)\n",
        "    except:\n",
        "        pass\n",
        "    if not list_posts_df:\n",
        "      print(\"no posts found!\")\n",
        "      return\n",
        "    new_posts = pd.concat(list_posts_df)\n",
        "    print(\"Number of posts: \", new_posts.shape[0])\n",
        "\n",
        "    # File output for posts\n",
        "    filename_csv = filename + \"REDDIT_POSTS_\" + datetime.now(tz=tz).strftime(\"%Y-%m-%dT%H-%M-%S%z\") + \".csv\"\n",
        "    new_posts.to_csv(str(RAW_DATA_DIRECTORY / filename_csv), index=False)\n",
        "    print(\"Exported posts to CSV\")\n",
        "\n",
        "    # Get comments\n",
        "    post_ids = new_posts['post_id'].to_list()\n",
        "    list_comment_dfs = []\n",
        "    i = 0\n",
        "    for post in post_ids:\n",
        "        try:\n",
        "            comment_df = get_reddit_comments(r, post)\n",
        "            list_comment_dfs.append(comment_df)\n",
        "            # print(i, \"Grabbed\", len(comment_df), \"comments from post id:\", post)\n",
        "        except:\n",
        "            pass\n",
        "        i += 1\n",
        "    post_comments = pd.concat(list_comment_dfs)\n",
        "    print(\"Number of total comments: \", post_comments.shape[0])\n",
        "\n",
        "    # File output for comment\n",
        "    com_filename_csv = filename + \"REDDIT_COMMENTS_\" + datetime.now(tz=tz).strftime(\"%Y-%m-%dT%H-%M-%S%z\") + \".csv\"\n",
        "    post_comments.to_csv(str(RAW_DATA_DIRECTORY / com_filename_csv), index=False)\n",
        "    print(\"Exported comments to CSV\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksCIeVbBdBeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11bc41d-b9f6-47c9-f848-7e92cb76c92a"
      },
      "source": [
        "collect(input_subreddits=\"highschool Highschool_Help teenagers HighschoolTheater\", filename=\"highschool\")\n",
        "    \n",
        "client_id = \"zZ50cezaVccRCA\"\n",
        "client_secret = \"\t0maHmZXgAS2oDhO4t0eoYRLzumyWNA\"\n",
        "user_agent = \"PostMasterGeneral\"\n",
        "searchterms = \"biden\"\n",
        "reddits = \"politics uspolitics PoliticalDiscussion geopolitics freethought changemyview news worldnews government politics2\"\n",
        "reddits = \"highschool Highschool_Help teenagers HighschoolTheater\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter client secret: ··········\n",
            "Enter search terms (seperated by spaces): \n",
            "Successful token\n",
            "Grabbed 999 posts from subreddit: highschool\n",
            "Grabbed 260 posts from subreddit: Highschool_Help\n",
            "Grabbed 994 posts from subreddit: teenagers\n",
            "Grabbed 145 posts from subreddit: HighschoolTheater\n",
            "Number of posts:  2398\n",
            "Exported posts to CSV\n",
            "Number of total comments:  7352\n",
            "Exported comments to CSV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRQwEcjc1zup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a690594-7d8e-4172-89bb-b9c1ec9d88fd"
      },
      "source": [
        "collect(input_subreddits=\"politics uspolitics PoliticalDiscussion geopolitics freethought changemyview news worldnews government politics2\", filename=\"politics\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter client secret: ··········\n",
            "Enter search terms (seperated by spaces): \n",
            "Successful token\n",
            "Grabbed 940 posts from subreddit: politics\n",
            "Grabbed 999 posts from subreddit: uspolitics\n",
            "Grabbed 965 posts from subreddit: PoliticalDiscussion\n",
            "Grabbed 801 posts from subreddit: geopolitics\n",
            "Grabbed 956 posts from subreddit: freethought\n",
            "Grabbed 985 posts from subreddit: changemyview\n",
            "Grabbed 159 posts from subreddit: news\n",
            "Grabbed 294 posts from subreddit: worldnews\n",
            "Grabbed 504 posts from subreddit: government\n",
            "Grabbed 973 posts from subreddit: politics2\n",
            "Number of posts:  7576\n",
            "Exported posts to CSV\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:prawcore:Retrying due to 503 status: GET https://oauth.reddit.com/comments/jg2818/\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of total comments:  81081\n",
            "Exported comments to CSV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOcE3gQfsRVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f76c4b-d238-4580-ebcb-1f86f6f2e3a4"
      },
      "source": [
        "collect(input_subreddits=\"GradSchool PhD PhDStress LawSchool\", filename=\"grad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter client secret: ··········\n",
            "Enter search terms (seperated by spaces): \n",
            "Successful token\n",
            "Grabbed 981 posts from subreddit: GradSchool\n",
            "Grabbed 980 posts from subreddit: PhD\n",
            "Grabbed 212 posts from subreddit: PhDStress\n",
            "Grabbed 999 posts from subreddit: LawSchool\n",
            "Number of posts:  3172\n",
            "Exported posts to CSV\n",
            "Number of total comments:  14879\n",
            "Exported comments to CSV\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}