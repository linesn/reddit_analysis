{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Newspaper_Search-2.0.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linesn/reddit_analysis/blob/main/Notebooks/Newspaper_Search-2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "k_UvatCHH6P5"
      },
      "source": [
        "# News searching\n",
        "*Nick Lines*\n",
        "\n",
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#System-dependent-Configuration\" data-toc-modified-id=\"System-dependent-Configuration-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>System-dependent Configuration</a></span></li></ul></li><li><span><a href=\"#Collect-Data\" data-toc-modified-id=\"Collect-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Collect Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collect-Newspaper-Articles\" data-toc-modified-id=\"Collect-Newspaper-Articles-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Collect Newspaper Articles</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GohLFtksH6QD"
      },
      "source": [
        "# Introduction\n",
        "This notebook is adapted from a playbook provided by the Discovery Lab, Applied Intelligence, Accenture Federal Services. While the notebook you are reading is original work, I credit the Discovery Lab playbook for pointing out all the packages and libraries in use.\n",
        "\n",
        "The purpose of this notebook is to allow the user to search for a specific term or set of terms over a given time period, and receive the google news headlines that were found to be associated with that search. These are output as a CSV. The notebook is designed to run either in Google Colab or on a desktop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "242LrDR6H6QE"
      },
      "source": [
        "# Setup\n",
        "\n",
        "\n",
        "<p> The imports, function and class defintions, global variables, and system-dependent configuration are in this section. </p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cBLJle5H6QG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cWuoWPQXfS3"
      },
      "source": [
        "The first few cells are essential to run this notebook in colab. All other imports are usually already available in the colab environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onAQDBrDIBLC"
      },
      "source": [
        "try:\r\n",
        "  from selenium import webdriver\r\n",
        "  from selenium.common.exceptions import StaleElementReferenceException\r\n",
        "  from selenium.webdriver.common.keys import Keys\r\n",
        "  from selenium.webdriver.chrome.options import Options\r\n",
        "  from selenium.webdriver.support.ui import WebDriverWait\r\n",
        "except:\r\n",
        "  !pip install selenium\r\n",
        "  from selenium import webdriver\r\n",
        "  from selenium.common.exceptions import StaleElementReferenceException\r\n",
        "  from selenium.webdriver.common.keys import Keys\r\n",
        "  from selenium.webdriver.chrome.options import Options\r\n",
        "  from selenium.webdriver.support.ui import WebDriverWait "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHej55HOXBiI"
      },
      "source": [
        "try:\r\n",
        "  from newspaper import Article, fulltext\r\n",
        "except:\r\n",
        "  !pip install newspaper3k\r\n",
        "  from newspaper import Article, fulltext"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw8YTfgmX8dX"
      },
      "source": [
        "try:\r\n",
        "  from GoogleNews import GoogleNews\r\n",
        "except:\r\n",
        "  !pip install GoogleNews  \r\n",
        "  from GoogleNews import GoogleNews"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcM3juXbYePf"
      },
      "source": [
        "try:\r\n",
        "  from unidecode import unidecode\r\n",
        "except:\r\n",
        "  !pip install unidecode\r\n",
        "  from unidecode import unidecode"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-rMwaFZ2H6QG",
        "outputId": "0517ec19-8d01-44d0-da7c-597996afb95f"
      },
      "source": [
        "\"\"\"This cell imports necessary Python modules and performs initial configuration\n",
        "\"\"\"\n",
        "\n",
        "### Data manipulation libraries\n",
        "# import json\n",
        "import pandas as pd \n",
        "#import csv\n",
        "\n",
        "### Visualization and Interaction\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.style.use('ggplot')\n",
        "\n",
        "from IPython.display import set_matplotlib_formats, display, clear_output, HTML\n",
        "set_matplotlib_formats('retina')\n",
        "\n",
        "#import plotly.graph_objs as go\n",
        "#from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
        "#init_notebook_mode(connected=True)\n",
        "\n",
        "#import ipywidgets as widgets\n",
        "#from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "#from ipywidgets import VBox, HBox, Button, HTML, Label\n",
        "\n",
        "### Computation libraries \n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "\n",
        "### Graph analysis\n",
        "# import networkx as nx\n",
        "# import community\n",
        "\n",
        "### System related\n",
        "# import warnings;\n",
        "# warnings.filterwarnings('ignore')\n",
        "import io\n",
        "import os\n",
        "import platform\n",
        "from pathlib import Path\n",
        "import sys\n",
        "# from joblib import Parallel, delayed\n",
        "\n",
        "### Datetime libraries\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "from pytz import timezone\n",
        "\n",
        "### NLP dependencies\n",
        "# import spacy\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "# nlp = spacy.load('en')\n",
        "# tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "# from langdetect import detect\n",
        "\n",
        "### Scraping libraries\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "### Machine learning libraries\n",
        "# from sklearn import datasets\n",
        "# from sklearn import linear_model\n",
        "# from sklearn.feature_selection import f_regression, mutual_info_regression\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "### Logging\n",
        "import logging \n",
        "logging.basicConfig(level=logging.INFO)\n",
        "log = logging.getLogger(__name__)\n",
        "#import spacy\n",
        "# nlp = spacy.load('en')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RmsRB9HH6QZ"
      },
      "source": [
        "## System-dependent Configuration\r\n",
        "This cell allows the user to save the results of the query in Google Drive if the notebook is running on Colab, or locally if they are running this on their own machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whYVNvcsH6Qg",
        "outputId": "6745e7b0-227e-4c91-f434-b0f2f8bcd027"
      },
      "source": [
        "\"\"\"This cell defines system-dependent configuration such as those different in Linux vs. Windows\n",
        "\"\"\"\n",
        "if 'COLAB_GPU' in os.environ: # a hacky way of determining if you are in colab.\n",
        "  print(\"Notebook is running in colab\")\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "  OUTPUT_DIR = \"./drive/My Drive/Data/raw/\"\n",
        "  \n",
        "else:\n",
        "  # Get the system information from the OS\n",
        "  PLATFORM_SYSTEM = platform.system()\n",
        "\n",
        "  # Darwin is macOS\n",
        "  if PLATFORM_SYSTEM == \"Darwin\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver\")\n",
        "  elif PLATFORM_SYSTEM == \"Windows\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver.exe\")\n",
        "  else:\n",
        "      logging.critical(\"Chromedriver not found or Chromedriver is outdated...\")\n",
        "      exit()\n",
        "  OUTPUT_DIR = \"../Data/raw/\"\n",
        "output_file = OUTPUT_DIR + \"articles_output.csv\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Notebook is running in colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-uOLkXKH6QL"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb_hNJVTH6QL"
      },
      "source": [
        "\"\"\"This cell defines global variables and parameters used throughout the playbook\n",
        "\"\"\"\n",
        "\n",
        "# Set this to True if you want to watch Selenium scrape pages\n",
        "WATCH_SCRAPING = True\n",
        "\n",
        "# Set this to True if you want to use incognito mode\n",
        "USE_INCOGNITO = True\n",
        "\n",
        "# The data is written \n",
        "# RAW_DATA_DIRECTORY = Path(\"../data/raw/\")\n",
        "\n",
        "# Setup logging level\n",
        "LOGGING_LEVEL = logging.INFO \n",
        "logging.basicConfig(level=LOGGING_LEVEL)\n",
        "start_date = \"02/02/2021\" # (datetime.today() - timedelta(days = 1)).strftime('%m/%d/%Y')\n",
        "end_date = \"02/03/2021\" # datetime.today().strftime('%m/%d/%Y')\n",
        "lang = \"en\"\n",
        "search_terms = \"biden\""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Vz-gB3H6QM"
      },
      "source": [
        "## Functions and Classes\r\n",
        "\r\n",
        "I did not use most of these, but have left them in case they are of use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwm9oT5fH6QN"
      },
      "source": [
        "\"\"\"This cell defines functions and classes used throughout the playbook\n",
        "\"\"\"\n",
        "\n",
        "# APIs\n",
        "import requests\n",
        "\n",
        "import random\n",
        "user_agent_list = [\n",
        "   #Chrome\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36','Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36','Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36','Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
        "    #Firefox\n",
        "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)','Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)','Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)','Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)','Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)','Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
        "]\n",
        "\n",
        "\n",
        "# logging\n",
        "import logging\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "# general python \n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import csv\n",
        "columns = ['url', 'body', 'title', 'summary', 'keywords', 'authors3k', 'pubdate', 'quotes', 'rel_sents']\n",
        "\n",
        "# nlp: download language package outside of shell before running\n",
        "# python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# multithreading\n",
        "import threading\n",
        "import queue\n",
        "global lck \n",
        "lck = threading.Lock()\n",
        "\n",
        "# start forreal\n",
        "\n",
        "\n",
        "\n",
        "def collect_urls(query, p_results, date_start, date_end, lang):\n",
        "    results_list = []\n",
        "    googlenews = GoogleNews(lang=lang, start=date_start, end=date_end) # format: '07/24/2020'\n",
        "    googlenews.search(query)\n",
        "    for i in range(min(p_results, )):\n",
        "        results_list.extend(googlenews.result())\n",
        "        googlenews.getpage(i+2)\n",
        "    \n",
        "    if False:\n",
        "        skips = ['videos'] # sites that don't contain article content\n",
        "        results_list = [item for item in results_list if not any([skip for skip in skips if skip in item])]\n",
        "    \n",
        "    log.info('links to attack: {}'.format(results_list))\n",
        "    return results_list\n",
        "\n",
        "\n",
        "class Worker(threading.Thread):\n",
        "\n",
        "\n",
        "    def __init__(self, q, i, *args, **kwargs):\n",
        "        self.q = q\n",
        "        self.i = i\n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "        \n",
        "    def run(self):\n",
        "        while True:\n",
        "            try:\n",
        "                j, url, keywords = self.q.get(timeout=4)  # 3s timeout\n",
        "                i = self.i\n",
        "            except queue.Empty:\n",
        "                return\n",
        "            \n",
        "            article_dict = {}\n",
        "            body = ''\n",
        "            try:\n",
        "                article_dict['url'] = url\n",
        "                log.info('[t{}] {}- processing\\n\\tlink: {}'.format(i, j, url))\n",
        "                # randomize user agents\n",
        "                user_agent = random.choice(user_agent_list)\n",
        "                headers = {'User-Agent': user_agent}\n",
        "                article = requests.get(url, headers=headers)\n",
        "                article_orig = article\n",
        "                # nespaper3k processing\n",
        "                article = Article(url)\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                article.nlp()\n",
        "                \n",
        "                article_dict['title'] = article.title\n",
        "                article_dict['summary'] = article.summary\n",
        "                article_dict['keywords'] = article.keywords\n",
        "                article_dict['authors3k'] = article.authors\n",
        "                article_dict['pubdate'] = article.publish_date\n",
        "                body = ''\n",
        "                try:\n",
        "                    body = article.text\n",
        "                except:\n",
        "                    body = 'fail'\n",
        "\n",
        "                try:\n",
        "                    html_body = article_orig.text\n",
        "                    body = fulltext(html_body)\n",
        "                except:\n",
        "                    body = 'fail'\n",
        "                    \n",
        "                article_dict['body'] = body\n",
        "                \n",
        "            except:\n",
        "                log.error('\\n\\n[t{}] {}- error: \\n\\turl: {}\\n\\tdetails [line {}]:{}\\n\\n'.format(i, j, url, sys.exc_info()[-1].tb_lineno, sys.exc_info()[0]))\n",
        "            \n",
        "            # extract quotes from body, remove stuff like l & r quotes first\n",
        "            body = unidecode(body)\n",
        "            if len(body) > 5:\n",
        "                # extract quotes\n",
        "                quotes = []\n",
        "                terms = ['said', 'say', 'state', 'argue', 'told', 'wrote', 'writ', 'tweet', 'announc']\n",
        "                for term in terms:\n",
        "                    r = re.compile(\n",
        "                        r'''(?:\"(?P<quote>[^\"]+)\"\\W+(?i:{0}\\w*)(?P<speaker>(?:\\s(?:he|she|they|[A-Z]+[a-z.]*))+)(?:\\s|(?:,(?P<title>(?:(?:\\s[\\w\\']+))+)[,.])))|(?:\"(?P<quote1>[^\"]+)\"(?P<speaker1>(?:\\s(?:he|she|they|(?:[A-Z]+[a-z.]*\\s)+))+)(?:\\s?|(?:,(?P<title1>(?:(?:\\s[\\w\\']+))+),))(?i:{0}\\w*))|(?:(?P<speaker2>(?:he|she|they|(?:[A-Z]+[a-z.]*\\s)+))(?:\\s?|(?:,(?P<title2>(?:\\s[\\w\\']+)+),\\s))(?i:{0}\\w*)\\s(?:\\w+\\s)*\"(?P<quote2>[^\"]+)\")|(?:(?i:{0}\\w*)(?P<speaker3>[^\"]+)\"(?P<quote3>[^\"]+)\"\\.)'''.format(term))\n",
        "                    re_dicts = [m.groupdict() for m in r.finditer(body)]\n",
        "                    for re_dict in re_dicts:\n",
        "                        clean_dict = {k: v for k, v in re_dict.items() if v}\n",
        "                        new_dict = {}\n",
        "                        for k, v in clean_dict.items():\n",
        "                            new_dict[re.sub(r'\\d+', '', str(k))] = v\n",
        "                        quotes.append(new_dict)\n",
        "                # extract relevant sentences \"about\" the subject\n",
        "                sentences = []\n",
        "                if len(keywords) > 0:\n",
        "                    for sentence in body.split('.'):\n",
        "                        doc = nlp(sentence)\n",
        "                        if any([keyword for keyword in keywords for token in doc if keyword in token.text.lower() and token.dep_ in ['dobj','attr']]):\n",
        "                            sentences.append(sentence)\n",
        "                if len(sentences) == 0:\n",
        "                    sentences = 'none'\n",
        "                        \n",
        "                # clean out empty regex returns\n",
        "                # quotes = [[item for item in tup if len(item)>0] for tup in quotes] # for tuples\n",
        "                # clean out html junk; obselete since using newspaper3k to grab body\n",
        "                # body = re.sub(r'#[\\w\\-\\s.:()]+{[^}]+}', '', body)\n",
        "                article_dict['quotes'] = json.dumps(quotes)\n",
        "                article_dict['rel_sents'] = sentences\n",
        "                log.info('[t{}] {}- success\\n\\turl: {}'.format(i, j, url))\n",
        "                \n",
        "            else:\n",
        "                log.error('[t{}] {}- unsuccessful: \\n\\turl: {}'.format(i, j, url))\n",
        "                for kee in columns:\n",
        "                    if kee not in article_dict: article_dict[kee] = 'fail'\n",
        "                \n",
        "            lck.acquire()\n",
        "            with open(output_file, 'a',encoding='utf-8-sig', newline='') as g:\n",
        "                csv.DictWriter(g, fieldnames=columns).writerow(article_dict)\n",
        "            lck.release()\n",
        "            log.info('[t{}] {}- written\\n\\tlink: {}'.format(i, j, url))\n",
        "            self.q.task_done()\n",
        "\n",
        "\n",
        "def process(query=None, urls=[], keywords = [], n_threads: int=40, p_results=1, \n",
        "             date_start=(datetime.today() - timedelta(days = 1)).strftime('%m/%d/%Y'), date_end=datetime.today().strftime('%m/%d/%Y'), lang='en'):\n",
        "    log.info('article scraping start time: {}'.format(datetime.now().strftime(\"%Y-%m-%d-%H.%M.%S\")))\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if (query and len(urls)>0) or (not query and len(urls)==0):\n",
        "        log.error('provide query or urls; not both or neither')\n",
        "        sys.exit(1)\n",
        "    \n",
        "    if query:\n",
        "        log.info('query: {}'.format(query))\n",
        "        urls = collect_urls(str(query), int(p_results), date_start, date_end, lang)\n",
        "        if len(urls) ==0:\n",
        "            log.error('no news results found')\n",
        "            sys.exit(1)\n",
        "        \n",
        "    log.info('URL count: {}'.format(len(urls)))\n",
        "    if len(urls) < n_threads: n_threads = round(len(urls)/4)\n",
        "\n",
        "    with open(output_file, 'w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
        "        writer.writeheader()\n",
        "    \n",
        "    q = queue.Queue()\n",
        "    for k, result in enumerate(urls):\n",
        "        q.put_nowait((k, result['link'], keywords))\n",
        "    for _ in range(n_threads):\n",
        "        Worker(q, _).start()\n",
        "        time.sleep(1)\n",
        "    q.join()\n",
        "    \n",
        "    log.info('article scraping finished. end time: {}'.format(datetime.now().strftime(\"%Y-%m-%d-%H.%M.%S\")))\n",
        "    log.info('article scraping completed in {}'.format(timedelta(seconds=int(time.time() - start_time))))\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "def literal_return(val):\n",
        "    try:\n",
        "        return literal_eval(val)\n",
        "    except (ValueError, SyntaxError) as e:\n",
        "        return val\n",
        "\n",
        "\n",
        "def position_process(keywords = None):\n",
        "    articles_df = pd.read_csv(output_file)\n",
        "    articles_df.quotes = articles_df.quotes.apply(lambda x: literal_return(str(x)))\n",
        "\n",
        "\n",
        "    # split quotes into columns\n",
        "    quote_df = articles_df[['url', 'quotes']]\n",
        "    quote_df = quote_df.quotes.apply(pd.Series) \\\n",
        "        .merge(quote_df, left_index = True, right_index = True) \\\n",
        "        .drop([\"quotes\"], axis = 1) \\\n",
        "        .melt(id_vars = ['url'], value_name = \"quote\") \\\n",
        "        .dropna() \\\n",
        "        .drop(\"variable\", axis = 1) \\\n",
        "        .reset_index(drop=True)\n",
        "        \n",
        "    # to explode dictionary\n",
        "    # quote_df = quote_df.rename(columns={'quote':'quote_dict'})\n",
        "    # quote_df.join(quote_df.quote_dict.apply(pd.Series))\n",
        "\n",
        "\n",
        "    # split sentences into columns\n",
        "    sent_df = articles_df[['url', 'rel_sents']]\n",
        "    sent_df = sent_df.rel_sents.apply(pd.Series) \\\n",
        "        .merge(sent_df, left_index = True, right_index = True) \\\n",
        "        .drop([\"rel_sents\"], axis = 1) \\\n",
        "        .melt(id_vars = ['url'], value_name = \"rel_sents\") \\\n",
        "        .dropna() \\\n",
        "        .drop(\"variable\", axis = 1) \\\n",
        "        .reset_index(drop=True)\n",
        "        \n",
        "    # select only the split columns' rows if others cannot be filtered out\n",
        "    # quote_df = quote_df[quote_df.variable.apply(lambda x: isinstance(x, (int)))]\n",
        "\n",
        "    # get rid of fails\n",
        "    quote_df = quote_df[quote_df.quote != 'fail']\n",
        "\n",
        "    # get rid of empties\n",
        "    sent_df = sent_df[sent_df.rel_sents != 'none']\n",
        "\n",
        "    def clean_keys(dict_cur):\n",
        "        new_dict = {}\n",
        "        for k, v in dict_cur.items():\n",
        "            new_dict[re.sub(r'\\d+', '', str(k))] = v\n",
        "        return new_dict\n",
        "\n",
        "    quote_df.quote = quote_df.quote.apply(clean_keys)\n",
        "\n",
        "    # source: https://medium.com/swlh/simple-sentiment-analysis-for-nlp-beginners-and-everyone-else-using-vader-and-textblob-728da3dbe33d\n",
        "    # setup:\n",
        "    # pip install -U textblob\n",
        "    # python -m textblob.download_corpora\n",
        "\n",
        "    # set sentiment column when keywords appear in the quote\n",
        "    quote_df['position'] = np.NaN\n",
        "    quote_df['position'] = quote_df.apply(lambda x: TextBlob(x['quote']['quote']).sentiment.polarity \\\n",
        "    # if not keywords or any([keyword for keyword in keywords if keyword in x['quote']['quote'].lower()]) \\\n",
        "    # else x['position']\n",
        "    , axis=1)\n",
        "    quote_df['speaker'] = quote_df['quote'].apply(lambda x: x['speaker'].strip())\n",
        "    quote_df['quote'] = quote_df['quote'].apply(lambda x: x['quote'].strip())\n",
        "    quote_df.to_csv('quotes.csv')\n",
        "    # or sentence\n",
        "    sent_df['position'] = np.NaN\n",
        "    sent_df['position'] = sent_df.apply(lambda x: TextBlob(x['rel_sents']).sentiment.polarity \\\n",
        "    if not keywords or any([keyword for keyword in keywords if keyword in x['rel_sents'].lower()]) \\\n",
        "    else x['position'], axis=1)\n",
        "    # quote_df['speaker'] = quote_df['rel_sents'].apply(lambda x: x['speaker'])\n",
        "    # quote_df['rel_sents'] = quote_df['rel_sents'].apply(lambda x: x['rel_sents'])\n",
        "    sent_df.to_csv('sentences.csv')\n",
        "\n",
        "    # TODO: quote_mean_df = quote_df.groupby(['url']).mean()\n",
        "    articles_df.to_csv('position_results.csv')\n",
        "    \"\"\"\n",
        "    quote_mean_df = quote_df.groupby(['url'])\n",
        "    try: \n",
        "        sent_mean_df = sent_df.groupby(['url']).mean()\n",
        "        df_means = pd.merge(quote_mean_df, sent_mean_df, on=['url'])\n",
        "        df_means['pos_mean'] = df_means.mean(axis=1)\n",
        "        \n",
        "    except: \n",
        "        df_means = quote_mean_df\n",
        "        \n",
        "    \n",
        "    df_final = pd.merge(articles_df, df_means, on=['url'])\n",
        "    df_final.to_csv('position_results.csv')\n",
        "    \"\"\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIcFkdVDnh6X"
      },
      "source": [
        "googlenews = GoogleNews(lang=lang, start=start_date, end=end_date) # format: '07/24/2020'\r\n",
        "googlenews.search(search_terms)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL7L-aSwn4Ma"
      },
      "source": [
        "results = googlenews.results()"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goPPATvCobG4"
      },
      "source": [
        "df = pd.DataFrame.from_records(data=results)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7maOIT_pUud"
      },
      "source": [
        "df.to_csv(output_file)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfBBliusH6Qh"
      },
      "source": [
        "# Collect Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcB12AJtH6Qh"
      },
      "source": [
        "## Collect Newspaper Articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7VrOVetH6Qi"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb26An5UfyR5",
        "outputId": "5e77abf2-0903-4304-c6bb-a410e1bec22c"
      },
      "source": [
        "!head ./drive/My\\ Drive/Data/raw/articles_output.csv"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ",title,media,date,datetime,desc,link,img\n",
            "0,\"FACT SHEET: President Biden Outlines Steps to Reform Our Immigration System by Keeping Families Together, Addressing the Root Causes of Irregular Migration, and Streamlining the Legal Immigration System\",,1 day ago,2021-02-02 20:39:50.560515,\"President Biden's strategy is centered on the basic premise that our country is safer, stronger, and more prosperous with a fair, safe and orderly immigration system ...\",https://www.whitehouse.gov/briefing-room/statements-releases/2021/02/02/fact-sheet-president-biden-outlines-steps-to-reform-our-immigration-system-by-keeping-families-together-addressing-the-root-causes-of-irregular-migration-and-streamlining-the-legal-immigration-syst/,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "1,Biden's Executive Orders: President to Sign 3 Rolling Back Trump's Immigration Agenda,The New York Times,th · 1 day ago,2021-02-02 20:39:50.662227,\"As a candidate, Mr. Biden vowed to once again welcome immigrants to American shores by quickly rolling back hundreds of actions by President Donald J. Trump ...\",https://www.nytimes.com/2021/02/02/us/politics/biden-immigration-executive-orders-trump.html,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "2,Biden signs immigration executive orders to address 'moral failing' of Trump's policies,NBC News,21 hours ago,2021-02-02 23:39:50.664375,\"President Biden's approach is to deal with immigration comprehensively, fairly and humanely.\"\" After signing the orders in the White House on Tuesday, Biden told ...\",https://www.nbcnews.com/politics/immigration/biden-sign-executive-orders-immigration-including-family-reunification-n1256431,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "3,\"Biden issues new immigration orders, while signaling cautious approach\",Washington Post,19 hours ago,2021-02-03 01:39:50.666362,President Biden signed executive actions Tuesday ordering the review and potential reversal of the Trump administration's deterrent policies along the Mexico ...,https://www.washingtonpost.com/national/biden-immigration-executive-order/2021/02/02/8c7510a8-64f3-11eb-bf81-c618c88ed605_story.html,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "4,Biden signs immigration executive orders and establishes task force to reunite separated families,CNN,th · 20 hours ago,2021-02-03 00:39:50.766568,MUST WATCH. Hear Joe Biden's immigration plan 00:57. (CNN) President Joe Biden signed three executive orders Tuesday that take aim at his predecessor's ...,https://www.cnn.com/2021/02/02/politics/biden-immigration-executive-orders/index.html,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "5,Biden Signs Executive Orders Reversing Trump Immigration ...,Voice of America,8 hours ago,2021-02-03 12:39:50.768650,\"Biden's immediate focus is on the 3,100-kilometer southern border with Mexico, where Trump tried to keep thousands of migrants from Mexico, Honduras, El ...\",https://www.voanews.com/usa/biden-signs-executive-orders-reversing-trump-immigration-policies,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "6,\"Biden, in first trade move, reimposes a Trump tariff\",Politico,23 hours ago,2021-02-02 21:39:50.770548,A White House spokesperson indicated foreign policy concerns were a major factor behind Biden's move. President Joe Biden in the Oval Office of the White ...,https://www.politico.com/news/2021/02/01/biden-aluminum-tariff-uae-464794,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "7,\"FACT SHEET: President Biden Announces Increased Vaccine Supply, Initial Launch of the Federal Retail Pharmacy Program, and Expansion of FEMA Reimbursement to States\",,1 day ago,2021-02-02 20:39:50.772282,\"FACT SHEET: President Biden Announces Increased Vaccine Supply, Initial Launch of the Federal Retail Pharmacy Program, and Expansion of FEMA ...\",https://www.whitehouse.gov/briefing-room/statements-releases/2021/02/02/fact-sheet-president-biden-announces-increased-vaccine-supply-initial-launch-of-the-federal-retail-pharmacy-program-and-expansion-of-fema-reimbursement-to-states/,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n",
            "8,Biden administration to ship COVID vaccines to retail ...,WRCB-TV,1 day ago,2021-02-02 20:39:50.774055,\"The initial shipment of 1 million doses will go to 6,500 stores starting Feb. 11, said Jeffrey Zients, the Biden administration's Covid-19 coordinator. States were left ...\",https://www.wrcbtv.com/story/43278937/biden-administration-to-ship-covid-vaccines-to-retail-pharmacies-next-week,\"data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WE6kso2tH6Qi",
        "outputId": "4ed1636c-52ee-4a9e-e4d7-ffc1e7e37494"
      },
      "source": [
        "\"\"\"Add post-processing steps here\n",
        "\"\"\"\n",
        "\n",
        "# Clean up the environment\n",
        "# driver.quit()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Add post-processing steps here\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}