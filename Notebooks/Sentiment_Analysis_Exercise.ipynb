{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/linesn/reddit_analysis/blob/main/Sentiment_Analysis_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdEoGm_zorEa"
   },
   "source": [
    "# Sentiment Analysis Exercise \n",
    "Nicholas Lines  \n",
    "EN.605.633.81.SP21 Social Media Analytics  \n",
    "\n",
    "## Introduction\n",
    "This notebook is my response to the prompt to train a sentiment analysis classifier using [NLTK](https://www.nltk.org/) and [the Sentiment140 corpus](http://help.sentiment140.com/for-students), which was introduced in [1]. As instructed, we'll follow [the tutorial](https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/) written by Laurent Luce. The result (both of the tutorial and what we'll make here) is a document (tweet) level binary positive/negative classifier using bag-of-words features. The resulting classifier is purely for learning and demonstration purposes, and is not fit for use in real applications -- for real English language sentiment analysis projects I recommend [VADER](https://github.com/cjhutto/vaderSentiment) for lexical rules-based decisions at the sentence or document level, or something like [the Stanford NLP approach](https://nlp.stanford.edu/sentiment/) for supervised modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV_ipJ9wquwM"
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfEFoEcltlfG",
    "outputId": "cfda6242-196c-43f7-ce53-fd704410ed62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ebZWly14yDs5"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import langdetect\n",
    "except:\n",
    "  ! pip install langdetect\n",
    "  import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6U8kKY17G0S",
    "outputId": "8b866b43-375f-441e-fbcb-c4878a3bd9c8"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from vaderSentiment import vaderSentiment\n",
    "except:\n",
    "  ! pip install vaderSentiment\n",
    "  from vaderSentiment import vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2gVe3pLXpL1j"
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4Rd71MKkyTAM"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A0fl5ZY2eist"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Mx_0h2HybwOy"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hY4chCrKtqxA",
    "outputId": "3b0bdaa7-8384-4b38-ba5d-cdb2c928c661"
   },
   "outputs": [],
   "source": [
    "if 'COLAB_GPU' in os.environ: # a hacky way of determining if you are in colab.\n",
    "  print(\"Notebook is running in colab\")\n",
    "  from google.colab import drive\n",
    "  drive.mount(\"/content/drive\")\n",
    "  DATA_DIR = \"drive/MyDrive/Data/raw/\"\n",
    "  \n",
    "else:\n",
    "  try:\n",
    "    # Get the system information from the OS\n",
    "    PLATFORM_SYSTEM = platform.system()\n",
    "\n",
    "    # Darwin is macOS\n",
    "    if PLATFORM_SYSTEM == \"Darwin\":\n",
    "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver\")\n",
    "    elif PLATFORM_SYSTEM == \"Windows\":\n",
    "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver.exe\")\n",
    "    else:\n",
    "      logging.critical(\"Chromedriver not found or Chromedriver is outdated...\")\n",
    "      exit()\n",
    "    DATA_DIR = \"../Data/\"\n",
    "  except:\n",
    "    DATA_DIR = \"../Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvNj5fVLqxqI"
   },
   "source": [
    "## Getting and preparing the training data\n",
    "Note that the Sentiment140 data was gathered by querying Twitter for tweets including a given word (e.g. product name) AND emoticons that were used to declare a tweet Positive or Negative. The raw data is kept unchanged except the emoticons are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hsWqC2bnt8Qj"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_DIR + \"/training.1600000.processed.noemoticon.csv\"):\n",
    "  ! wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "  ! unzip trainingandtestdata.zip -d $DATA_DIR\n",
    "  ! ls $DATA_DIR -lrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH422W6fw3cD"
   },
   "source": [
    "The tweets in the corpus are labeled as follows  \n",
    "\n",
    "| number | meaning  |  \n",
    "| ------ | -------- |  \n",
    "| 0      | negative |\n",
    "| 2      | neutral  |\n",
    "| 4      | positive |\n",
    "\n",
    "In practice, though, the data seems to only include the negative and positive tweets. I also notice that there does not appear to be any language filtration in place, so we would want to add that in a real-life application with mixed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "U-XmuT9wyth1",
    "outputId": "ce949024-0850-4208-8f52-79d2d4522982"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity    tweet_id                          date     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = [\"polarity\", \"tweet_id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "df = pd.read_csv(DATA_DIR+\"training.1600000.processed.noemoticon.csv\", parse_dates=True, names=header, encoding=\"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yu7r4Ti-1wMz",
    "outputId": "470bd39a-8c64-47e8-cab1-031a76d8cbf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLIC8lYI1zHi",
    "outputId": "8db20b85-ad0c-4d2c-9b9d-edc666aebaf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfyIrbmv3EOt",
    "outputId": "4c73577e-c152-44b5-bb31-4b5fa8352447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138    29850\n",
       "137    22142\n",
       "136    18793\n",
       "48     16652\n",
       "46     16616\n",
       "       ...  \n",
       "243        1\n",
       "244        1\n",
       "248        1\n",
       "252        1\n",
       "374        1\n",
       "Length: 257, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = array([len(i) for i in df[\"text\"]])\n",
    "pd.value_counts(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9W2PWQr8_VV7",
    "outputId": "a0ea411d-0d58-41cb-9636-b20c0f5e3bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity    False\n",
       "tweet_id    False\n",
       "date        False\n",
       "query       False\n",
       "user        False\n",
       "text        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okXKcZAeBBZb"
   },
   "source": [
    "The text encoding (`latin-1`) is inconvenient, so we'll change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "N9ygwIorACWP"
   },
   "outputs": [],
   "source": [
    "df.text = df.text.apply (lambda row: row.encode(\"utf-8\", \"ignore\").decode('utf-8','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n_nuE9zU7h-9"
   },
   "outputs": [],
   "source": [
    "# df[\"lang\"] = df.text.apply (lambda row: langdetect.detect(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4KNtt81oCElz"
   },
   "outputs": [],
   "source": [
    "negatives = df[df[\"polarity\"]==0].text.to_numpy()\n",
    "positives = df[df[\"polarity\"]==4].text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2kvKiDBjZouf"
   },
   "outputs": [],
   "source": [
    "percent_train = .80\n",
    "plim = int(len(positives) * percent_train)\n",
    "nlim = int(len(negatives) * percent_train)\n",
    "positives_train = positives[:plim]\n",
    "positives_test = positives[plim:]\n",
    "negatives_train = negatives[:nlim]\n",
    "negatives_test = negatives[nlim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "rILESIsRCbBy"
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for words in positives_train:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    tweets.append((words_filtered, 'positive'))\n",
    "for words in negatives_train:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    tweets.append((words_filtered, 'negative'))\n",
    "test_tweets = []\n",
    "for words in positives_test:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    test_tweets.append((words_filtered, 'positive'))\n",
    "for words in negatives_test:\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    test_tweets.append((words_filtered, 'negative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckNNRk2E2gX1"
   },
   "source": [
    "At this point we have to deviate from the tutorial heavily. The tutorial uses every vocabulary word as a feature, which is just not feasible when we have 1,191,165 vocabulary words (the result of including Twitter handles, no doubt). At my estimation, using all possible features and all of the training data would have taken about 10 weeks to complete the training. \n",
    "\n",
    "So we'll do some simple vocabulary pruning. We drop all terms that do not appear at least 10 times (since they won't help identify trends) and all terms that appear extremely frequently, using the minimum of the second differences in the ordered frequency counts to find an elbow. This drastically reduces our vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "sUwRCayxC3mN"
   },
   "outputs": [],
   "source": [
    "def elbowcut(list_object):\n",
    "  list_object = sorted(list_object, reverse=True)\n",
    "  d = [list_object[i+1]-list_object[i] for i in range(len(list_object)-1)]\n",
    "  dd = [d[i+1]-d[i] for i in range(len(d) - 1)] \n",
    "  plot(dd)\n",
    "  return list_object[argmin(dd)+2]\n",
    "\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    goodkeys = []\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    #plot(sorted(list(wordlist.values()), reverse=True))\n",
    "    cutoff = elbowcut(list(wordlist.values()))\n",
    "    for key in wordlist:\n",
    "      if wordlist[key] > 10 and wordlist[key] < cutoff:\n",
    "        goodkeys.append(key) \n",
    "    word_features = set(goodkeys)\n",
    "    return word_features\n",
    "\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "shtT2jMmbizk",
    "outputId": "2fec37eb-16a1-4e14-c87c-11836786c815"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_features = get_word_features(get_words_in_tweets(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rL_V8dBqxAPg",
    "outputId": "9f717066-5944-4819-8840-684fe072d851"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48101"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "QXJke7OGcWrF"
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "testing_set = nltk.classify.apply_features(extract_features, test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "TUEVBAP8cBVr"
   },
   "outputs": [],
   "source": [
    "shortset = training_set[:10] + training_set[int(len(training_set)/2):int(len(training_set)/2+10)]\n",
    "longset = training_set[:20] + training_set[int(len(training_set)/2):int(len(training_set)/2+20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLGCrMn7qxok"
   },
   "source": [
    "## Building the classifier, training, and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVPMLt6rYlN"
   },
   "source": [
    "Let's see how long training takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHiB7ciVdPSp",
    "outputId": "490544f3-eb0a-40a3-f010-86b6f78170f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.72 s, sys: 406 ms, total: 4.12 s\n",
      "Wall time: 4.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier = nltk.NaiveBayesClassifier.train(shortset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpbCm-hJgW3V",
    "outputId": "191e7ce1-89cc-4aeb-d364-afc490f09534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 344 ms, total: 4.69 s\n",
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier = nltk.NaiveBayesClassifier.train(longset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fY0iultPiDaX",
    "outputId": "5b97e235-1ffd-4e0b-ad44-ddb43ef33145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the 40 training iterations timing,\n",
      " the full computation should be done in 2400.0 minutes\n"
     ]
    }
   ],
   "source": [
    "t = (4.5 / 40) * df.shape[0] * percent_train / 60 # time in minutes\n",
    "print(\"Given the 40 training iterations timing,\\n\\\n",
    " the full computation should be done in\", t, \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQJAlrHU4ZDQ"
   },
   "source": [
    "That is still more time than I can justify, so we'll have to only use part of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kp9q0gHrref"
   },
   "source": [
    "If we were to go ahead and use this training method (perhaps on better hardware?) for the whole training dataset, we'd run the cells commented out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RtZxsaWLel4u"
   },
   "outputs": [],
   "source": [
    "# classifier = nltk.NaiveBayesClassifier.train(longset)\n",
    "# with open(DATA_DIR+\"/classifier.pkl\", 'wb') as outfile:\n",
    "#   pickle.dump(classifier, outfile) # If you train this all the way, you don't want to do it twice!\n",
    "# print(classifier.classify(extract_features(\"this is a dumb sentence\".split())))\n",
    "# classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_XFHXGI6fdv4"
   },
   "outputs": [],
   "source": [
    "# print(\"Training accuracy:\", nltk.classify.accuracy(classifier, training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GTnbiZKDqkWs"
   },
   "outputs": [],
   "source": [
    "# print(\"Testing accuracy:\",nltk.classify.accuracy(classifier, testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Varl-kn4Xi2",
    "outputId": "93c2e354-6734-4399-c22d-c102415694b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 5 minutes I can train about 2666 iterations\n"
     ]
    }
   ],
   "source": [
    "print(\"In 5 minutes I can train about\", int(5 * 40/(4.5/60)), \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVJWoMv-qjos"
   },
   "source": [
    "Training on a subset of the full training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ECfqXXk4stjn"
   },
   "outputs": [],
   "source": [
    "my_training = training_set[:500] + training_set[int(len(training_set)/2):int(len(training_set)/2+500)]\n",
    "my_test = testing_set[:100] + testing_set[int(len(testing_set)/2):int(len(testing_set)/2+100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyCT9ulk51Mk",
    "outputId": "dfb5a7e3-3906-41b4-b69b-e09f781b6a0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 344 ms, total: 1min 22s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier = nltk.NaiveBayesClassifier.train(my_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "WlGcAWCX9MIU"
   },
   "outputs": [],
   "source": [
    "with open(DATA_DIR+\"/classifier.pkl\", 'wb') as outfile:\n",
    "    pickle.dump(classifier, outfile) # If you train this all the way, you don't want to do it twice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYw1-4HD8feW"
   },
   "outputs": [],
   "source": [
    "with open(DATA_DIR+\"/classifier.pkl\", 'rb') as infile:\n",
    "    classifier, df = pickle.load(infile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKl4C0dGq7L6"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmcYxsq_6huJ",
    "outputId": "02be192a-c0dc-43da-8e1b-ef4797d1c791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "Most Informative Features\n",
      "           contains(sad) = True           negati : positi =     12.3 : 1.0\n",
      "        contains(pretty) = True           positi : negati =      6.3 : 1.0\n",
      "        contains(thanks) = True           positi : negati =      6.1 : 1.0\n",
      "           contains(bad) = True           negati : positi =      5.8 : 1.0\n",
      "           contains(me,) = True           negati : positi =      5.7 : 1.0\n",
      "        contains(missed) = True           negati : positi =      5.7 : 1.0\n",
      "       contains(morning) = True           positi : negati =      5.0 : 1.0\n",
      "          contains(poor) = True           negati : positi =      5.0 : 1.0\n",
      "         contains(happy) = True           positi : negati =      4.6 : 1.0\n",
      "contains(@jonathanrknight) = True           positi : negati =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(extract_features(\"this is a dumb sentence\".split())))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_pnhHTM76nyA"
   },
   "outputs": [],
   "source": [
    "# print(\"Training accuracy:\", nltk.classify.accuracy(classifier, my_training))\n",
    "# Causes a memory crash, for no reason I can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "AyQLSlkU6rwH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.715\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing accuracy:\",nltk.classify.accuracy(classifier, my_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, 71.5% accuracy is not very impressive. How does this compare to a popular standard library like Vader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = vaderSentiment.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_raw = test_tweets[:100] + test_tweets[int(len(testing_set)/2):int(len(testing_set)/2+100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = 0\n",
    "for tweet in my_test_raw:\n",
    "    if analyzer.polarity_scores(\" \".join(tweet[0]))['compound']>0:\n",
    "        result=\"positive\"\n",
    "    else:\n",
    "        result=\"negative\"\n",
    "    if result == tweet[1]:\n",
    "        matches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "print(\"Vader accuracy:\", matches/len(my_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, Vader does a little bit worse than our wimpy little classifier in this very small test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQURwan2q9i2"
   },
   "source": [
    "## Results  \n",
    "There are a few big lessons learned here. One is that feature engineering is as important as classifier choice when setting up this model. There are probably smarter feature engineering choices that could be made that would improve my approach quite a bit. It also appears that NLTK's Naive Bayes classifier implementation is really slow and subject to memory issues -- even with as few as 1000 training iterations and the same number of classification steps it repeatedly crashed in Colaboratory and on my own box due to memory overflow. I'd like to compare it to the sklearn implementation at some point, since I've had good experiences with that code in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5veMaZqIyi"
   },
   "source": [
    "## References\n",
    "[1] Go, Alec, Richa Bhayani, and Lei Huang. \"Twitter sentiment classification using distant supervision.\" CS224N project report, Stanford 1.12 (2009): 2009.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJbE83704yvMBtiuQsiRmG",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Sentiment_Analysis_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
