
\documentclass{amsart}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsfonts}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Thursday, February 18, 2021 13:39:50}
%TCIDATA{LastRevised=Friday, April 23, 2021 01:05:28}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Articles\SW\AMS Journal Article">}
%TCIDATA{CSTFile=amsartci.cst}

\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{solution}{Solution}
\theoremstyle{plain}
\newtheorem{acknowledgement}{Acknowledgement}
\newtheorem{algorithm}{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem{conclusion}{Conclusion}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{criterion}{Criterion}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
\newtheorem{notation}{Notation}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{summary}{Summary}
\numberwithin{equation}{section}
\input{tcilatex}

\begin{document}
\title[Proposal: Vocabulary entropy among Reddit communities]{Social Media
Analysis\ Final Project Proposal: Vocabulary entropy among Reddit communities%
}
\author{Nicholas Lines}
\email{nicholasalines@gmail.com}
\date{\today }
\maketitle

\section{Introduction}

In this document I present a proposal for my final project, which is focused
on vocabulary entropy among Reddit communties, focused primarily on
political discussion subreddits. It is not a draft of the paper, but is
intended to help me explore the sections I'll be writing. This discussion is
motivated by a desire to augment distance-reading methods like Topic
Modeling with language-agnostic techniques for profiling a community of
writers from a distance. In my final project I'll ask whether political
discourse on Reddit is most similar to high-school level, graduate-school
level, or non-native English writing, using as my metric the mean vocabulary
entropy in samples taken using the Python Reddit API PRAW. My intended
peer-review process is to submit this to KDD.

\section{Background}

Use of vocabulary to profile writers or groups of writers is commonplace,
but fraught with difficulties. While it is generally acknowledged that
vocabulary size is correlated with fluency and education levels,
sub-exhaustive methods for approximating vocabulary size are difficult find.
One possible solution is to instead measure vocabulary entropy in a
document. Suppose we have a document that is $n$ words long composed using $V
$ distinct words $X_{1},X_{2},...,X_{V}$. These words's frequencies are
observed and used to write the vocabulary's empirical probability
distribution for the random variable for word choice, $X$. The Shannon
entropy of the document is defined to be%
\begin{equation*}
H(X)=-\sum_{i=1}^{V}P(X_{i})\log _{10}P(X_{i}).
\end{equation*}%
This provides an excellent measure of the spread of vocabulary use in the
document, and intuitively more entropy indicates broader vocabulary
knowledge, which is associated with better education and language skills.
However, vocabulary use in writing follows Heap's law, which states that the
number of unique words observed $V$ in a document of length $n$ words is
given by%
\begin{equation*}
V(n)=\alpha n^{\beta },
\end{equation*}%
where $\alpha $ and $\beta $ are parameters dependent on the author and
possibly the language and context of the document. This means that document
length will greatly affect the entropy measure. Suggestions have been made
for how to resolve this problem so that comparisons can be made between
documents of varying lengths. In \cite{rajput2018novel} the authors put
forward "a novel approach" for normalizing the vocabulary entropy term, in
which they simply divide it by the maximum entropy of a document of the same
size, $-\log _{10}\left( 1/n\right) $, and call the result the vocabulary
quotient. It turns out that this exact method was also advocated in \cite[p.
551]{dale2000handbook}. Unfortunately, it can easily be shown that the
vocabulary quotient is unstable, and indeed that it strictly decreases as $n$
increases. As a result, we will restrict our review to universal fixed
lengths of $n=100$ words. We'll continue to use vocabulary quotients rather
than the unnormalized Shannon entropy since it provides a minimal amount of
consistency.

\section{Data}

I have gathered posts and comments from Reddit in four main categories:\ a
sample from Redditors for whom English is a second language, a sample from
high school related subreddits (i.e. text written by high school students),
a sample from graduate school related subreddits, and finally a much larger
sample from a variety of US political discussion subreddits. From these I
kept only the posts or comments that were 100 tokens long or more after
removing punctuation and tokenizing. Each item is cut down to just the first
100 tokens, resulting in the sample sizes shown in Table \ref{TableKey}.

%TCIMACRO{\TeXButton{B}{\begin{table}[tbp] \centering}}%
%BeginExpansion
\begin{table}[tbp] \centering%
%EndExpansion
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{English as a second language} & \textbf{High school} & \textbf{%
Graduate School} & \textbf{US Politics} \\ \hline
534 & 923 & 4260 & 20411 \\ \hline
\end{tabular}%
\caption{Subreddit group sample sizes}\label{TableKey}%
%TCIMACRO{\TeXButton{E}{\end{table}} }%
%BeginExpansion
\end{table}
%EndExpansion

\section{Methodology}

The research question to be tested is whether the the average vocabulary
quotient of Reddit political discussions is more similar to that of graduate
students or one of the less entropic groups. I've started some of this, but
still have work to do. So far I've shown that the mean vocabulary quotient
for each population is different, with the expected result that graduate
students have higher entropy in their text than high school students, and
the English learners have the least diverse vocabulary. However, I've been
surprised to see that the US Politics vocabulary quotient is better even
than graduate school text. These results are summarized in a box plot in
Figure \ref{ve}. I will need to show that these differences are
statistically significant. Next I'd like to break the political data out by
subreddit and see if there are some subreddits more prone to lower
vocabulary quotients. If I have time, I'd also like to take examine a social
network whose nodes are Redditors whose posts/comments in US\ Politics
subreddits appeared in the dataset, but including edges from shorter
messages as well; if possible I'd like to investigate the relation of node
centrality and vocabulary quotient.   \FRAME{ftbpFU}{3.7758in}{3.717in}{0pt}{%
\Qcb{Vocabulary quotients in several subreddit groups.}}{\Qlb{ve}}{%
vocabulary_entropy_box_plot.jpg}{\special{language "Scientific Word";type
"GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width
3.7758in;height 3.717in;depth 0pt;original-width 7.0992in;original-height
6.986in;cropleft "0";croptop "1";cropright "1";cropbottom "0";filename
'images/vocabulary_entropy_box_plot.jpg';file-properties "XNPEU";}}

I am also a big believer in open research and reproducibility, so I want to
provide all my code and relevant material in a GitHub repository that is
world viewable. 

\section{Questions for the instructor}

I have a couple areas I'd appreciate guidance or suggestions in. I'd like to
provide a copy of the data that I used, but I also want it to be sanitized.
Do you think it is sufficient for me to simply provide token index lists
rather than the raw data? Admittedly this is just a substitution cipher
using numbers for words, so a lot can be recovered based on frequency
tables, and confirmed using public Reddit data. But the data is all public
on Reddit anyway, so I don't think that is a big deal. I also am looking for
suggestions on where to host the data. In a pinch I can keep it in the
GitHub repository, but that seems less professional than hosting it
somewhere standard.

\bibliographystyle{amsplain}
\bibliography{acompat,JHU}

\end{document}
