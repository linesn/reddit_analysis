{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3qtQLtZDpwqnN+GWKz/lk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linesn/reddit_analysis/blob/main/Sentiment_Analysis_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdEoGm_zorEa"
      },
      "source": [
        "# Sentiment Analysis Exercise \n",
        "Nicholas Lines  \n",
        "EN.605.633.81.SP21 Social Media Analytics  \n",
        "\n",
        "## Introduction\n",
        "This notebook is my response to the prompt to train a sentiment analysis classifier using [NLTK](https://www.nltk.org/) and [the Sentiment140 corpus](http://help.sentiment140.com/for-students), which was introduced in [1]. As instructed, we'll follow [the tutorial](https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/) written by Laurent Luce. The result (both of the tutorial and what we'll make here) is a document (tweet) level binary positive/negative classifier using bag-of-words features. The resulting classifier is purely for learning and demonstration purposes, and is not fit for use in real applications -- for real English language sentiment analysis projects I recommend [VADER](https://github.com/cjhutto/vaderSentiment) for lexical rules-based decisions at the sentence or document level, or something like [the Stanford NLP approach](https://nlp.stanford.edu/sentiment/) for supervised modeling. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV_ipJ9wquwM"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfEFoEcltlfG",
        "outputId": "65c9aaa9-4490-42da-a7f0-9d3158b19f3a"
      },
      "source": [
        "%pylab inline\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebZWly14yDs5",
        "outputId": "24624c8c-bfba-4484-a850-26a15edc6fdb"
      },
      "source": [
        "try:\n",
        "  import langdetect\n",
        "except:\n",
        "  ! pip install langdetect\n",
        "  import langdetect"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\r\u001b[K     |▍                               | 10kB 14.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 7.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112kB 7.3MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 174kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 194kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 204kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 225kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 235kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 256kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 266kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 983kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp37-none-any.whl size=993193 sha256=2aebdcee5c82f9ca0b76ddc3b49b47ca983d4b63a0d69be0fcd86683a1071787\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rd71MKkyTAM"
      },
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0fl5ZY2eist"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx_0h2HybwOy"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY4chCrKtqxA",
        "outputId": "12bc9055-3551-4834-e05b-5787b11562de"
      },
      "source": [
        "if 'COLAB_GPU' in os.environ: # a hacky way of determining if you are in colab.\n",
        "  print(\"Notebook is running in colab\")\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "  DATA_DIR = \"drive/MyDrive/Data/raw/\"\n",
        "  \n",
        "else:\n",
        "  # Get the system information from the OS\n",
        "  PLATFORM_SYSTEM = platform.system()\n",
        "\n",
        "  # Darwin is macOS\n",
        "  if PLATFORM_SYSTEM == \"Darwin\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver\")\n",
        "  elif PLATFORM_SYSTEM == \"Windows\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver.exe\")\n",
        "  else:\n",
        "      logging.critical(\"Chromedriver not found or Chromedriver is outdated...\")\n",
        "      exit()\n",
        "  DATA_DIR = \"../Data/\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Notebook is running in colab\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvNj5fVLqxqI"
      },
      "source": [
        "## Getting and preparing the training data\n",
        "Note that the Sentiment140 data was gathered by querying Twitter for tweets including a given word (e.g. product name) AND emoticons that were used to declare a tweet Positive or Negative. The raw data is kept unchanged except the emoticons are removed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsWqC2bnt8Qj"
      },
      "source": [
        "if not os.path.exists(DATA_DIR + \"/training.1600000.processed.noemoticon.csv\"):\n",
        "  ! wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "  ! unzip trainingandtestdata.zip -d $DATA_DIR\n",
        "  ! ls $DATA_DIR -lrt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH422W6fw3cD"
      },
      "source": [
        "The tweets in the corpus are labeled as follows  \n",
        "\n",
        "| number | meaning  |  \n",
        "| ------ | -------- |  \n",
        "| 0      | negative |\n",
        "| 2      | neutral  |\n",
        "| 4      | positive |\n",
        "\n",
        "In practice, though, the data seems to only include the negative and positive tweets. I also notice that there does not appear to be any language filtration in place, so we would want to add that in a real-life application with mixed data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "U-XmuT9wyth1",
        "outputId": "74563903-2bd1-4773-8ced-54c1ffb492d4"
      },
      "source": [
        "header = [\"polarity\", \"tweet_id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "df = pd.read_csv(DATA_DIR+\"training.1600000.processed.noemoticon.csv\", parse_dates=True, names=header, encoding=\"latin-1\")\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1         0  ...  is upset that he can't update his Facebook by ...\n",
              "2         0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3         0  ...    my whole body feels itchy and like its on fire \n",
              "4         0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu7r4Ti-1wMz",
        "outputId": "c57ddedc-783a-4482-ce35-3ff93c56b599"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600000, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLIC8lYI1zHi",
        "outputId": "ff6866b7-624b-47b9-a3f5-c469c8b350b4"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    800000\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfyIrbmv3EOt",
        "outputId": "7f3ff10b-7dd1-4e86-bca6-63fc0481a68b"
      },
      "source": [
        "lengths = array([len(i) for i in df[\"text\"]])\n",
        "pd.value_counts(lengths)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138    29850\n",
              "137    22142\n",
              "136    18793\n",
              "48     16652\n",
              "46     16616\n",
              "       ...  \n",
              "243        1\n",
              "244        1\n",
              "248        1\n",
              "252        1\n",
              "374        1\n",
              "Length: 257, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W2PWQr8_VV7",
        "outputId": "aa14bbe9-dfb0-4aa6-a549-76e20fbafde6"
      },
      "source": [
        "df.isnull().any()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "polarity    False\n",
              "tweet_id    False\n",
              "date        False\n",
              "query       False\n",
              "user        False\n",
              "text        False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okXKcZAeBBZb"
      },
      "source": [
        "The text encoding (`latin-1`) is inconvenient, so we'll change that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9ygwIorACWP"
      },
      "source": [
        "df.text = df.text.apply (lambda row: row.encode(\"utf-8\", \"ignore\").decode('utf-8','ignore'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_nuE9zU7h-9"
      },
      "source": [
        "# df[\"lang\"] = df.text.apply (lambda row: langdetect.detect(row))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KNtt81oCElz"
      },
      "source": [
        "negatives = df[df[\"polarity\"]==0].text.to_numpy()\n",
        "positives = df[df[\"polarity\"]==4].text.to_numpy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kvKiDBjZouf"
      },
      "source": [
        "percent_test = .80\n",
        "plim = int(len(positives) * percent_test)\n",
        "nlim = int(len(negatives) * percent_test)\n",
        "positives_train = positives[:plim]\n",
        "positives_test = positives[plim:]\n",
        "negatives_train = negatives[:nlim]\n",
        "negatives_test = negatives[nlim:]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rILESIsRCbBy"
      },
      "source": [
        "tweets = []\n",
        "for words in positives_train:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'positive'))\n",
        "for words in negatives_train:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'negative'))\n",
        "test_tweets = []\n",
        "for words in positives_test:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'positive'))\n",
        "for words in negatives_test:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'negative'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUwRCayxC3mN"
      },
      "source": [
        "def get_words_in_tweets(tweets):\n",
        "    all_words = []\n",
        "    for (words, sentiment) in tweets:\n",
        "      all_words.extend(words)\n",
        "    return all_words\n",
        "\n",
        "\n",
        "def get_word_features(wordlist):\n",
        "    wordlist = nltk.FreqDist(wordlist)\n",
        "    word_features = wordlist.keys()\n",
        "    return word_features\n",
        "\n",
        "\n",
        "def extract_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains(%s)' % word] = (word in document_words)\n",
        "    return features"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shtT2jMmbizk"
      },
      "source": [
        "word_features = get_word_features(get_words_in_tweets(tweets))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXJke7OGcWrF"
      },
      "source": [
        "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
        "testing_set = nltk.classify.apply_features(extract_features, test_tweets)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUEVBAP8cBVr"
      },
      "source": [
        "shortset = training_set[:10] + training_set[int(len(training_set)/2):int(len(training_set)/2+10)]\n",
        "longset = training_set[:20] + training_set[int(len(training_set)/2):int(len(training_set)/2+20)]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLGCrMn7qxok"
      },
      "source": [
        "## Building the classifier and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHiB7ciVdPSp",
        "outputId": "1b023062-e92c-459c-f2ad-af3dbf505643"
      },
      "source": [
        "%%time\n",
        "classifier = nltk.NaiveBayesClassifier.train(shortset)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 22s, sys: 1.61 s, total: 1min 24s\n",
            "Wall time: 1min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpbCm-hJgW3V",
        "outputId": "5a555ef6-4718-4100-a82f-1937c79227c7"
      },
      "source": [
        "%%time\n",
        "classifier = nltk.NaiveBayesClassifier.train(longset)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 27s, sys: 2.49 s, total: 2min 29s\n",
            "Wall time: 2min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY0iultPiDaX",
        "outputId": "658aded7-f64a-4de3-ad72-222c206c1877"
      },
      "source": [
        "((2 +27/60)/40)*len(training_set)/60/24/7"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.722222222222223"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtZxsaWLel4u"
      },
      "source": [
        "with open(DATA_DIR+\"/classifier.pkl\", 'wb') as outfile:\n",
        "  pickle.dump(classifier, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_XFHXGI6fdv4",
        "outputId": "b65f1038-da8e-41ae-cdc0-26e187d7e35d"
      },
      "source": [
        "classifier.classify(extract_features(\"this is a dumb sentence\".split()))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKl4C0dGq7L6"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30AxXy69fMfk",
        "outputId": "f7d9d11c-0499-474a-aba3-ffc6102ed6e8"
      },
      "source": [
        "classifier.show_most_informative_features()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "          contains(your) = True           positi : negati =      2.3 : 1.0\n",
            "          contains(with) = True           negati : positi =      2.3 : 1.0\n",
            "           contains(you) = True           positi : negati =      1.8 : 1.0\n",
            "       contains(friends) = True           negati : positi =      1.7 : 1.0\n",
            "           contains(for) = False          negati : positi =      1.6 : 1.0\n",
            "           contains(the) = False          negati : positi =      1.4 : 1.0\n",
            "           contains(you) = False          negati : positi =      1.3 : 1.0\n",
            "          contains(with) = False          positi : negati =      1.3 : 1.0\n",
            "          contains(your) = False          negati : positi =      1.3 : 1.0\n",
            "           contains(who) = False          positi : negati =      1.2 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZCfN0c0deqV",
        "outputId": "36c0b815-c024-4292-d38f-f0c27803a2a3"
      },
      "source": [
        "nltk.classify.accuracy(classifier, shortset)\n",
        "#print(label_probdist.prob('positive'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQURwan2q9i2"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E71Z0B_xoWoc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a5veMaZqIyi"
      },
      "source": [
        "## References\n",
        "[1] Go, Alec, Richa Bhayani, and Lei Huang. \"Twitter sentiment classification using distant supervision.\" CS224N project report, Stanford 1.12 (2009): 2009.\n"
      ]
    }
  ]
}