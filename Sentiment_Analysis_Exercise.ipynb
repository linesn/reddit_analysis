{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJbE83704yvMBtiuQsiRmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linesn/reddit_analysis/blob/main/Sentiment_Analysis_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdEoGm_zorEa"
      },
      "source": [
        "# Sentiment Analysis Exercise \n",
        "Nicholas Lines  \n",
        "EN.605.633.81.SP21 Social Media Analytics  \n",
        "\n",
        "## Introduction\n",
        "This notebook is my response to the prompt to train a sentiment analysis classifier using [NLTK](https://www.nltk.org/) and [the Sentiment140 corpus](http://help.sentiment140.com/for-students), which was introduced in [1]. As instructed, we'll follow [the tutorial](https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/) written by Laurent Luce. The result (both of the tutorial and what we'll make here) is a document (tweet) level binary positive/negative classifier using bag-of-words features. The resulting classifier is purely for learning and demonstration purposes, and is not fit for use in real applications -- for real English language sentiment analysis projects I recommend [VADER](https://github.com/cjhutto/vaderSentiment) for lexical rules-based decisions at the sentence or document level, or something like [the Stanford NLP approach](https://nlp.stanford.edu/sentiment/) for supervised modeling. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV_ipJ9wquwM"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfEFoEcltlfG",
        "outputId": "cfda6242-196c-43f7-ce53-fd704410ed62"
      },
      "source": [
        "%pylab inline\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebZWly14yDs5"
      },
      "source": [
        "try:\n",
        "  import langdetect\n",
        "except:\n",
        "  ! pip install langdetect\n",
        "  import langdetect"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6U8kKY17G0S",
        "outputId": "8b866b43-375f-441e-fbcb-c4878a3bd9c8"
      },
      "source": [
        "try:\n",
        "  from vaderSentiment import vaderSentiment\n",
        "except:\n",
        "  ! pip install vaderSentiment\n",
        "  from vaderSentiment import vaderSentiment"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gVe3pLXpL1j"
      },
      "source": [
        "# from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rd71MKkyTAM"
      },
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0fl5ZY2eist"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx_0h2HybwOy"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY4chCrKtqxA",
        "outputId": "3b0bdaa7-8384-4b38-ba5d-cdb2c928c661"
      },
      "source": [
        "if 'COLAB_GPU' in os.environ: # a hacky way of determining if you are in colab.\n",
        "  print(\"Notebook is running in colab\")\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "  DATA_DIR = \"drive/MyDrive/Data/raw/\"\n",
        "  \n",
        "else:\n",
        "  # Get the system information from the OS\n",
        "  PLATFORM_SYSTEM = platform.system()\n",
        "\n",
        "  # Darwin is macOS\n",
        "  if PLATFORM_SYSTEM == \"Darwin\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver\")\n",
        "  elif PLATFORM_SYSTEM == \"Windows\":\n",
        "      EXECUTABLE_PATH = Path(\"../dependencies/chromedriver.exe\")\n",
        "  else:\n",
        "      logging.critical(\"Chromedriver not found or Chromedriver is outdated...\")\n",
        "      exit()\n",
        "  DATA_DIR = \"../Data/\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Notebook is running in colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvNj5fVLqxqI"
      },
      "source": [
        "## Getting and preparing the training data\n",
        "Note that the Sentiment140 data was gathered by querying Twitter for tweets including a given word (e.g. product name) AND emoticons that were used to declare a tweet Positive or Negative. The raw data is kept unchanged except the emoticons are removed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsWqC2bnt8Qj"
      },
      "source": [
        "if not os.path.exists(DATA_DIR + \"/training.1600000.processed.noemoticon.csv\"):\n",
        "  ! wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "  ! unzip trainingandtestdata.zip -d $DATA_DIR\n",
        "  ! ls $DATA_DIR -lrt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH422W6fw3cD"
      },
      "source": [
        "The tweets in the corpus are labeled as follows  \n",
        "\n",
        "| number | meaning  |  \n",
        "| ------ | -------- |  \n",
        "| 0      | negative |\n",
        "| 2      | neutral  |\n",
        "| 4      | positive |\n",
        "\n",
        "In practice, though, the data seems to only include the negative and positive tweets. I also notice that there does not appear to be any language filtration in place, so we would want to add that in a real-life application with mixed data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "U-XmuT9wyth1",
        "outputId": "ce949024-0850-4208-8f52-79d2d4522982"
      },
      "source": [
        "header = [\"polarity\", \"tweet_id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "df = pd.read_csv(DATA_DIR+\"training.1600000.processed.noemoticon.csv\", parse_dates=True, names=header, encoding=\"latin-1\")\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1         0  ...  is upset that he can't update his Facebook by ...\n",
              "2         0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3         0  ...    my whole body feels itchy and like its on fire \n",
              "4         0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu7r4Ti-1wMz",
        "outputId": "470bd39a-8c64-47e8-cab1-031a76d8cbf6"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600000, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLIC8lYI1zHi",
        "outputId": "8db20b85-ad0c-4d2c-9b9d-edc666aebaf7"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    800000\n",
              "0    800000\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfyIrbmv3EOt",
        "outputId": "4c73577e-c152-44b5-bb31-4b5fa8352447"
      },
      "source": [
        "lengths = array([len(i) for i in df[\"text\"]])\n",
        "pd.value_counts(lengths)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138    29850\n",
              "137    22142\n",
              "136    18793\n",
              "48     16652\n",
              "46     16616\n",
              "       ...  \n",
              "243        1\n",
              "244        1\n",
              "248        1\n",
              "252        1\n",
              "374        1\n",
              "Length: 257, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W2PWQr8_VV7",
        "outputId": "a0ea411d-0d58-41cb-9636-b20c0f5e3bd6"
      },
      "source": [
        "df.isnull().any()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "polarity    False\n",
              "tweet_id    False\n",
              "date        False\n",
              "query       False\n",
              "user        False\n",
              "text        False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okXKcZAeBBZb"
      },
      "source": [
        "The text encoding (`latin-1`) is inconvenient, so we'll change that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9ygwIorACWP"
      },
      "source": [
        "df.text = df.text.apply (lambda row: row.encode(\"utf-8\", \"ignore\").decode('utf-8','ignore'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_nuE9zU7h-9"
      },
      "source": [
        "# df[\"lang\"] = df.text.apply (lambda row: langdetect.detect(row))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KNtt81oCElz"
      },
      "source": [
        "negatives = df[df[\"polarity\"]==0].text.to_numpy()\n",
        "positives = df[df[\"polarity\"]==4].text.to_numpy()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kvKiDBjZouf"
      },
      "source": [
        "percent_train = .80\n",
        "plim = int(len(positives) * percent_train)\n",
        "nlim = int(len(negatives) * percent_train)\n",
        "positives_train = positives[:plim]\n",
        "positives_test = positives[plim:]\n",
        "negatives_train = negatives[:nlim]\n",
        "negatives_test = negatives[nlim:]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rILESIsRCbBy"
      },
      "source": [
        "tweets = []\n",
        "for words in positives_train:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'positive'))\n",
        "for words in negatives_train:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'negative'))\n",
        "test_tweets = []\n",
        "for words in positives_test:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'positive'))\n",
        "for words in negatives_test:\n",
        "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
        "    tweets.append((words_filtered, 'negative'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckNNRk2E2gX1"
      },
      "source": [
        "At this point we have to deviate from the tutorial heavily. The tutorial uses every vocabulary word as a feature, which is just not feasible when we have 1,191,165 vocabulary words (the result of including Twitter handles, no doubt). At my estimation, using all possible features and all of the training data would have taken about 10 weeks to complete the training. \n",
        "\n",
        "So we'll do some simple vocabulary pruning. We drop all terms that do not appear at least 5 times (since they won't help identify trends) and all terms that appear extremely frequently, using the minimum of the second differences in the ordered frequency counts to find an elbow. This drastically reduces our vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUwRCayxC3mN"
      },
      "source": [
        "def elbowcut(list_object):\n",
        "  list_object = sorted(list_object, reverse=True)\n",
        "  d = [list_object[i+1]-list_object[i] for i in range(len(list_object)-1)]\n",
        "  dd = [d[i+1]-d[i] for i in range(len(d) - 1)] \n",
        "  plot(dd)\n",
        "  return list_object[argmin(dd)+2]\n",
        "\n",
        "\n",
        "def get_words_in_tweets(tweets):\n",
        "    all_words = []\n",
        "    for (words, sentiment) in tweets:\n",
        "      all_words.extend(words)\n",
        "    return all_words\n",
        "\n",
        "\n",
        "def get_word_features(wordlist):\n",
        "    goodkeys = []\n",
        "    wordlist = nltk.FreqDist(wordlist)\n",
        "    #plot(sorted(list(wordlist.values()), reverse=True))\n",
        "    cutoff = elbowcut(list(wordlist.values()))\n",
        "    for key in wordlist:\n",
        "      if wordlist[key] > 5 and wordlist[key] < cutoff:\n",
        "        goodkeys.append(key) \n",
        "    word_features = set(goodkeys)\n",
        "    return word_features\n",
        "\n",
        "\n",
        "def extract_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['contains(%s)' % word] = (word in document_words)\n",
        "    return features"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shtT2jMmbizk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "2fec37eb-16a1-4e14-c87c-11836786c815"
      },
      "source": [
        "word_features = get_word_features(get_words_in_tweets(tweets))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL_V8dBqxAPg",
        "outputId": "9f717066-5944-4819-8840-684fe072d851"
      },
      "source": [
        "len(word_features)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100198"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXJke7OGcWrF"
      },
      "source": [
        "training_set = nltk.classify.apply_features(extract_features, tweets)\n",
        "testing_set = nltk.classify.apply_features(extract_features, test_tweets)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUEVBAP8cBVr"
      },
      "source": [
        "shortset = training_set[:10] + training_set[int(len(training_set)/2):int(len(training_set)/2+10)]\n",
        "longset = training_set[:20] + training_set[int(len(training_set)/2):int(len(training_set)/2+20)]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLGCrMn7qxok"
      },
      "source": [
        "## Building the classifier, training, and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVPMLt6rYlN"
      },
      "source": [
        "Let's see how long training takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHiB7ciVdPSp",
        "outputId": "490544f3-eb0a-40a3-f010-86b6f78170f7"
      },
      "source": [
        "%%time\n",
        "classifier = nltk.NaiveBayesClassifier.train(shortset)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.12 s, sys: 263 ms, total: 9.38 s\n",
            "Wall time: 9.45 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpbCm-hJgW3V",
        "outputId": "191e7ce1-89cc-4aeb-d364-afc490f09534"
      },
      "source": [
        "%%time\n",
        "classifier = nltk.NaiveBayesClassifier.train(longset)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 16.4 s, sys: 112 ms, total: 16.5 s\n",
            "Wall time: 16.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fY0iultPiDaX",
        "outputId": "5b97e235-1ffd-4e0b-ad44-ddb43ef33145"
      },
      "source": [
        "t = (16.5 / 40) * df.shape[0] * percent_train / 60 # time in minutes\n",
        "print(\"Given the 40 training iterations timing,\\n\\\n",
        " the full computation should be done in\", t, \"minutes\")"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given the 40 training iterations timing,\n",
            " the full computation should be done in 8800.0 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQJAlrHU4ZDQ"
      },
      "source": [
        "That is still more time than I can justify, so we'll have to only use part of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kp9q0gHrref"
      },
      "source": [
        "If we were to go ahead and use this training method (perhaps on better hardware?) for the whole training dataset, we'd run the cells commented out below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtZxsaWLel4u"
      },
      "source": [
        "# classifier = nltk.NaiveBayesClassifier.train(longset)\n",
        "# with open(DATA_DIR+\"/classifier.pkl\", 'wb') as outfile:\n",
        "#   pickle.dump(classifier, outfile) # If you train this all the way, you don't want to do it twice!\n",
        "# print(classifier.classify(extract_features(\"this is a dumb sentence\".split())))\n",
        "# classifier.show_most_informative_features()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XFHXGI6fdv4"
      },
      "source": [
        "# print(\"Training accuracy:\", nltk.classify.accuracy(classifier, training_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTnbiZKDqkWs"
      },
      "source": [
        "# print(\"Testing accuracy:\",nltk.classify.accuracy(classifier, testing_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Varl-kn4Xi2",
        "outputId": "93c2e354-6734-4399-c22d-c102415694b5"
      },
      "source": [
        "print(\"In 10 minutes I can train about\", int(10 * 40/(16.5/60)), \"iterations\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In 10 minutes I can train about 1454 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVJWoMv-qjos"
      },
      "source": [
        "Training on a subset of the full training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECfqXXk4stjn"
      },
      "source": [
        "my_training = training_set[:500] + training_set[int(len(training_set)/2):int(len(training_set)/2+500)]\n",
        "my_test = testing_set[:100] + testing_set[int(len(testing_set)/2):int(len(testing_set)/2+100)]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyCT9ulk51Mk",
        "outputId": "dfb5a7e3-3906-41b4-b69b-e09f781b6a0c"
      },
      "source": [
        "%%time\n",
        "classifier = nltk.NaiveBayesClassifier.train(my_training)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5min 25s, sys: 849 ms, total: 5min 26s\n",
            "Wall time: 5min 27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYw1-4HD8feW"
      },
      "source": [
        "with open(DATA_DIR+\"/classifier.pkl\", 'rb') as infile:\n",
        "  classifier = pickle.load(infile) "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKl4C0dGq7L6"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlGcAWCX9MIU"
      },
      "source": [
        "with open(DATA_DIR+\"/classifier.pkl\", 'wb') as outfile:\n",
        "  pickle.dump(classifier, outfile) # If you train this all the way, you don't want to do it twice!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmcYxsq_6huJ",
        "outputId": "02be192a-c0dc-43da-8e1b-ef4797d1c791"
      },
      "source": [
        "print(classifier.classify(extract_features(\"this is a dumb sentence\".split())))\n",
        "classifier.show_most_informative_features()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n",
            "Most Informative Features\n",
            "        contains(thanks) = True           positi : negati =     14.3 : 1.0\n",
            "           contains(sad) = True           negati : positi =     11.7 : 1.0\n",
            "         contains(never) = True           negati : positi =      9.0 : 1.0\n",
            "          contains(glad) = True           positi : negati =      8.3 : 1.0\n",
            "         contains(sucks) = True           negati : positi =      6.3 : 1.0\n",
            "         contains(cause) = True           negati : positi =      5.7 : 1.0\n",
            "          contains(too!) = True           positi : negati =      5.7 : 1.0\n",
            "           contains(bad) = True           negati : positi =      5.4 : 1.0\n",
            "           contains(hey) = True           positi : negati =      5.0 : 1.0\n",
            "         contains(didnt) = True           negati : positi =      5.0 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pnhHTM76nyA"
      },
      "source": [
        "print(\"Training accuracy:\", nltk.classify.accuracy(classifier, my_training))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyQLSlkU6rwH"
      },
      "source": [
        "print(\"Testing accuracy:\",nltk.classify.accuracy(classifier, my_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQURwan2q9i2"
      },
      "source": [
        "## Results  \n",
        "There are a few big lessons learned here. One is that feature engineering is as important as classifier choice when setting up this model. There are probably smarter feature engineering choices that could be made that would improve my approach quite a bit. It also appears that NLTK's Naive Bayes classifier implementation is really slow. I'd like to compare it to the sklearn implementation if I had more time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a5veMaZqIyi"
      },
      "source": [
        "## References\n",
        "[1] Go, Alec, Richa Bhayani, and Lei Huang. \"Twitter sentiment classification using distant supervision.\" CS224N project report, Stanford 1.12 (2009): 2009.\n"
      ]
    }
  ]
}